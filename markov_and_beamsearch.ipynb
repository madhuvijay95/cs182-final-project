{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model and Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4643 clickbait articles.\n",
      "   Unnamed: 0                                      article_title                                        article_url  clickbait    source\n",
      "0           0             23 Life Lessons Cosmo Kramer Taught Us  /javiermoreno/ife-lessons-you-learned-from-cos...          1  Buzzfeed\n",
      "1           1          32 Men On TV Who Made You Thirsty In 2014  /erinlarosa/32-tv-men-who-made-you-thirsty-in-...          1  Buzzfeed\n",
      "2           2          Hilary Duff Was The Walking Queen Of 2014  /lyapalater/hilary-duff-was-the-walking-queen-...          1  Buzzfeed\n",
      "3           3        25 Reasons Wine Is Definitely Your Soulmate  /emleschh/25-reasons-why-wine-is-your-soulmate...          1  Buzzfeed\n",
      "4           4  This Master Carver Making Pliers From One Stic...          /norbertobriceno/ernest-macguyver-warther          1  Buzzfeed\n"
     ]
    }
   ],
   "source": [
    "# import data from augmented.csv\n",
    "filename = 'augmented.csv'\n",
    "augmented_df = pd.read_csv(filename)\n",
    "gh_data = augmented_df[augmented_df['clickbait'] == 1]\n",
    "\n",
    "print 'There are', len(gh_data), 'clickbait articles.'\n",
    "print gh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Sanitize titles for Markov model, output each token on separate line in article_titles.txt ###\n",
    "\n",
    "def removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def removeQuotations(s):\n",
    "    counter = 0\n",
    "    for i in s:\n",
    "        if i == '\\\"':\n",
    "            counter += 1\n",
    "    return not (counter == 2 or counter == 0)\n",
    "\n",
    "def removeParens(s):\n",
    "    counter = 0\n",
    "    for i in s:\n",
    "        if i == '(' or i == ')':\n",
    "            counter += 1\n",
    "    return not (counter == 2 or counter == 0)\n",
    "\n",
    "titles = open('article_titles.txt', 'w')\n",
    "counter = 0\n",
    "for i in gh_data['article_title']:\n",
    "    nonAscii = removeNonAscii(i)\n",
    "    strlist = list(nonAscii)\n",
    "    lastchar = strlist[len(strlist) - 1]\n",
    "    if lastchar != '.' and lastchar != '!' and lastchar != '?':\n",
    "        strlist.append('.')\n",
    "    nonAscii = ''.join(strlist)\n",
    "    tokens = nonAscii.split()\n",
    "    if len(tokens) < 4:\n",
    "        continue\n",
    "    for token in tokens:\n",
    "        if removeQuotations(token):\n",
    "            token = token.replace('\\\"', \"\")\n",
    "        if removeParens(token):\n",
    "            token = token.replace('(', \"\")\n",
    "            token = token.replace(')', \"\")\n",
    "        if token.isupper():\n",
    "            if len(token) != 1 or token[0] == 'A':\n",
    "                token = token.lower()\n",
    "                tokenlist = list(token)\n",
    "                tokenlist[0] = tokenlist[0].upper()\n",
    "                token = \"\".join(tokenlist)\n",
    "        tokenlist = list(token)\n",
    "        tokenlist.append('\\n')\n",
    "        titles.write(''.join(tokenlist))\n",
    "titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53897\n",
      "5705\n"
     ]
    }
   ],
   "source": [
    "### Construct word-to-int mapping, set of existing titles for comparison below.  ###\n",
    "### Also this is kinda dumb but markovify expects 1 line == 1 sentence so write to markovify_titles.txt ###\n",
    "\n",
    "words = open('article_titles.txt', 'r')\n",
    "fullTitles = open('markovify_titles.txt', 'w')\n",
    "\n",
    "wordMap = {}\n",
    "reverseWordMap = {}\n",
    "existingTitles = []\n",
    "currentTitle = []\n",
    "totalWords = 0\n",
    "for line in words:\n",
    "    # Construct mapping of words to ints\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line not in wordMap:\n",
    "        wordMap[line] = len(wordMap)\n",
    "        reverseWordMap[wordMap[line]] = line\n",
    "    \n",
    "    # Construct set of article titles to test for similarity\n",
    "    currentTitle.append(wordMap[line])\n",
    "    lastchar = line[len(line)-1]\n",
    "    if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "        existingTitles.append(currentTitle)\n",
    "        totalWords += len(currentTitle)\n",
    "        fullTitles.write(\" \".join(map(lambda x: reverseWordMap[x], currentTitle) + ['\\n']))\n",
    "        currentTitle = []\n",
    "        \n",
    "words.close()\n",
    "fullTitles.close()\n",
    "\n",
    "print totalWords\n",
    "print len(existingTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Construct transition matrix of words, word frequency (mapping from word to count), and starter word frequency ###\n",
    "\n",
    "words = open('article_titles.txt', 'r')\n",
    "transitionMatrix = {}\n",
    "wordFrequency = {}\n",
    "beginnerFrequency = {}\n",
    "\n",
    "prevToken = None\n",
    "nextWordStart = True\n",
    "for line in words:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    \n",
    "    if line not in wordFrequency:\n",
    "        wordFrequency[line] = 0\n",
    "    wordFrequency[line] += 1\n",
    "    \n",
    "    if nextWordStart:\n",
    "        if line not in beginnerFrequency:\n",
    "            beginnerFrequency[line] = 0\n",
    "        beginnerFrequency[line] += 1\n",
    "        nextWordStart = False\n",
    "    \n",
    "    if prevToken == None:\n",
    "        prevToken = line\n",
    "    else:\n",
    "        if prevToken not in transitionMatrix:\n",
    "            transitionMatrix[prevToken] = {}\n",
    "        if line not in transitionMatrix[prevToken]:\n",
    "            transitionMatrix[prevToken][line] = 0\n",
    "        transitionMatrix[prevToken][line] += 1\n",
    "    \n",
    "    lastchar = line[len(line)-1]\n",
    "    if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "        prevToken = None\n",
    "        nextWordStart = True\n",
    "    else:\n",
    "        prevToken = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper functions for Markov model ###\n",
    "\n",
    "# Construct successor probability dist based on global word frequency\n",
    "def constructDist(keys):\n",
    "    frequencies = []\n",
    "    for key in keys:\n",
    "        frequencies.append(wordFrequency[key])\n",
    "    return map(lambda x: x / float(sum(frequencies)), frequencies)\n",
    "\n",
    "# Construct successor probability dist based on local word frequency\n",
    "def constructLocalDist(keys, frequencyTable):\n",
    "    frequencies = []\n",
    "    for key in keys:\n",
    "        frequencies.append(frequencyTable[key])\n",
    "    return map(lambda x: x / float(sum(frequencies)), frequencies)\n",
    "\n",
    "# Find fraction of title2 words appear in title1 (i.e. compare bags of words)\n",
    "def compareWords(title1, title2):\n",
    "    counter = 0\n",
    "    for i in title2:\n",
    "        if i in title1:\n",
    "            counter += 1\n",
    "    return float(counter) / len(title2)\n",
    "\n",
    "# Check for similarity against existing titles, return score of \"best\" (as in, most similar) existing title\n",
    "def checkTable(phrase):\n",
    "    # generate the list\n",
    "    newPhrase = []\n",
    "    tokens = phrase\n",
    "    for token in tokens:\n",
    "        newPhrase.append(wordMap[token])\n",
    "    \n",
    "    # Check against all preexisting titles\n",
    "    bestScore = 0\n",
    "    mostSimilar = None\n",
    "    for title in existingTitles:\n",
    "        score = compareWords(title, newPhrase)\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            mostSimilar = title\n",
    "    \n",
    "    return bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selena Gomez Spotted at these 21 Items I Have Over Pocahontas.\n",
      "'Orphan Black' Season 6 Photos In Class to Reevaluate Relationship.\n",
      "Duke Porn Star Kate Delights Kids on Juan Pablo Galavis' Video Removed Amidst Racism Issues.\n",
      "But a Woman Left Alone in Tears To Mark Wahlberg, Pharrell Gets Asked For College.\n",
      "Yesterdays Daily Battle...#11 Is Starting Her Birthday in Hollywood.\n",
      "International Space So Inappropriate, You'll Be Too High School Football To Step Inside...\n",
      "Arkansas Mom Found A Chinese Restaurant Foods You've Stood Under Anesthetic.\n",
      "'Clueless' Star Eli Wallach Dies From Nick ...\n",
      "Robin Thicke Drops Studio Version Of Animal Drawings Have No Other...OMG Is That Perfectly Adorable.\n",
      "Life And Other Dumbest Questions From ABC's World Travelers Understand.\n",
      "Future Teams to Direct 'Star Wars Revealed: Luke, Leia and a Move To?\n",
      "Scenes And 3 Pugs That We'll Never Walk His Lawyers.\n",
      "Whoopi Goldberg Adds Two Different Thing Of Gorillas.\n",
      "Zac Efron's Arms Did #4 So Awkward Dance Too.\n",
      "Picture of Medeival Technologies If Canada Is Staring Contest At First.\n",
      "'Game of SiriusXM, Is Helping Hand Moment...\n",
      "Wiz Khalifa Arrested in 'Happy' in 'The Casual Vacancy' Turned His Daughter Suri 'Overwhelming'.\n",
      "Harry Potter, Here's Why Draft Day For Real Message.\n",
      "'Sharknado 2' and Pip Andersen Join Judd Apatow Blast Washington Post Without Feeling Down #3.\n",
      "Unless Youre Under Water Tower From My Dreams.\n",
      "Game Amid His Word He Quit Marriage.\n",
      "Tweetdeck Got Stuck at 76.\n",
      "#7 Deserved That WIll Inevitably Result Is Everything.\n",
      "Let's Remember Christmas Tree Is Pissed Off Days...This Is Unfair.\n",
      "WikiLeaks Just Taking It Is...Fireball Should Really Hard At 65.\n",
      "Animal Was Banned In Hazmat Suits.\n",
      "Quentin Tarantino's 'Hateful Eight' May Not How Fast & Their Parents.\n",
      "Drastic Times: This Psychopath Watched Original Locations That Were Sending #17!\n",
      "Rare Albino Animals Loving & Peele.\n",
      "Jon Hamm Staring Contest At Google Should I Vow [updated].\n",
      "Bitcoins Will & More Beautiful Story.\n",
      "Patrick's Day Traditions From Time.com: The Minority Report Findings.\n",
      "Kfc Just Say Exactly Like Disney ...\n",
      "'Begin Again' From 'Amazing Spider- Man Walked Up Right on Public Apology at 91.\n",
      "Abc for 'i Have Missed.\n",
      "Who's A First Photo...But I'm Sure Your Haters.\n",
      "Afraid Of Arc Was Only Hilarious And It Werent For Women.\n",
      "Emoji That Send Them These Kids Really Put 30,000 Lights On Camera.\n",
      "Lana Del Rey's Boyfriend and Jonah Hill Addresses Racism Issues.\n",
      "On Child Stars' Saves the Studio, Record Label, Because The Line.\n",
      "Miley Cyrus Gets Out Full Costume Unveiled.\n",
      "#13 Looks Beautiful Model Poses With Him Your Tracks.\n",
      "'Cabin in Plans Midway Through the Men's Families.\n",
      "Zac Efron and Quinn's Kiss as Peter Pan in 20 Taxidermists Who Arent Around.\n",
      "Anita Baker Wanted for the Knot in Hawaii During Bath Time.\n",
      "Report: Will 'Feel Guilty Pleasure Is?\n",
      "Street Artist's Hand-Cut Paper Rolls Out.\n",
      "Rowling Publishes a Move to Back There?!\n",
      "Gisele Bundchen and 'Modern Family' Round Out Into Their Favorite Songs Ranked From Girls.\n",
      "Fantastic Staircases That Would Actually For You Speechless.\n"
     ]
    }
   ],
   "source": [
    "### Random walk through transition matrix, start from a starter word and end at punctuation (., !, ?)\n",
    "\n",
    "# tweakable parameters\n",
    "\n",
    "# min/max numbers of tokens per title\n",
    "minLength = 5\n",
    "maxLength = 15\n",
    "\n",
    "# number of titles to generate\n",
    "numTitles = 50\n",
    "\n",
    "# maximum allowed proportion of shared words b/t generated and preexisting title\n",
    "similarityThreshold = 0.5\n",
    "\n",
    "# don't touch these\n",
    "phrase = []\n",
    "prevWord = None\n",
    "counter = 0\n",
    "while True:\n",
    "    while True:\n",
    "        if prevWord == None:\n",
    "                keys = beginnerFrequency.keys()\n",
    "                word = np.random.choice(keys, 1, constructLocalDist(keys, beginnerFrequency))\n",
    "                phrase.append(word[0])\n",
    "                prevWord = word[0]\n",
    "        else:\n",
    "            if prevWord not in transitionMatrix:\n",
    "                phrase = []\n",
    "                prevWord = None\n",
    "            else:\n",
    "                keys = transitionMatrix[prevWord].keys()\n",
    "                word = np.random.choice(keys, 1, constructLocalDist(keys, transitionMatrix[prevWord]))\n",
    "                prevWord = word[0]\n",
    "                phrase.append(word[0])\n",
    "                lastchar = word[0][len(word[0])-1]\n",
    "                if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "                    break\n",
    "    if len(phrase) < minLength or len(phrase) > maxLength:\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    elif checkTable(phrase) > similarityThreshold:\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    else:\n",
    "        counter += 1\n",
    "        print \" \".join(phrase)\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    if counter > numTitles - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initialize Naive Bayes model ###\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from naive_bayes import NaiveBayes\n",
    "\n",
    "filename = 'augmented.csv'\n",
    "augmented_df = pd.read_csv(filename)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(augmented_df['article_title'])\n",
    "y = np.array(augmented_df['clickbait'])\n",
    "\n",
    "nb_madhu = NaiveBayes()\n",
    "nb_madhu.fit(X, y, vectorizer.vocabulary_, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Pics And Are Now For A Dog Photos And Tiny Animals Will Make ...\n",
      "21 Pics Of Dog Got It Look For Them Will Take It Wrong.\n",
      "21 Pics And Are Now For A Dog Are Some Magic To It Too Adorable.\n",
      "21 Pics And Are Now For A Dog Had to 5 Will Ever Existed.\n",
      "21 Pics Of Furniture Is Still Adorable.\n",
      "21 Pics Of Furniture Is Absolutely Changed Up A Tattoo in 20 Cats On You.\n",
      "21 Pics And Are Now For A Dog Photos I Started Out There.\n",
      "21 Pics Of Dog Got It Look For Them Will Take It Better?\n",
      "21 Pics And Are Now For A Dog Photos And Her Bully...\n",
      "21 Pics And Are Now For A Dog Are Some Magic To Look .\n",
      "21 Pics And Are Now For A Dog Photos And Now.\n",
      "21 Pics Of Furniture Is Adorable.\n",
      "21 Pics Of Dog Got It Look For #6.\n",
      "21 Pics And Are Now For A Dog Photos I Were Your Finger.\n",
      "21 Pics Of Dog Got It Look For And Drew All To Eachother.\n",
      "21 Pics Of Dog Got Involved.\n",
      "21 Pics Of Furniture Is Absolutely Changed Up A Tattoo in 20 Cats Got Approved!\n",
      "21 Pics Of Furniture Is Absolutely Changed Up A Tattoo in 20 Cats Are Stunning.\n",
      "21 Pics And Are Now For A Dog Had to Show The Sky.\n",
      "21 Pics And Are Now For Men Should The Food Forever.\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Beam search w/NB heuristic ###\n",
    "\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from math import exp\n",
    "\n",
    "# tweakable parameters\n",
    "minLength = 5\n",
    "maxLength = 15\n",
    "numTitles = 20\n",
    "beamWidth = 50\n",
    "\n",
    "# Construct successor probability distribution based on NB probability \n",
    "def constructBayesDist(phrase, keys):\n",
    "    probs = []\n",
    "    for key in keys:\n",
    "        newPhrase = phrase + [key]\n",
    "        result = nb_madhu.predict_proba(vectorizer.transform([\" \".join(newPhrase)]))\n",
    "        prob = result[0][1]\n",
    "        probs.append(prob)\n",
    "    # randomly weight each probability by e^(-.5x), where 0 <= x <= 1\n",
    "    probs = map(lambda x: x * exp(-.5 * random.random()), probs)\n",
    "    return probs\n",
    "\n",
    "prevNext = []\n",
    "currNext = []\n",
    "finishedCandidates = []\n",
    "firstLevel = True\n",
    "terminateMe = False\n",
    "\n",
    "while not terminateMe:\n",
    "    if firstLevel == True:\n",
    "        keys = beginnerFrequency.keys()\n",
    "        keyValList = zip([[] for i in range(len(keys))], keys, constructBayesDist([], keys))\n",
    "        keyValList.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        newBeamWidth = min(len(keyValList), beamWidth)\n",
    "        for i in range(newBeamWidth):\n",
    "            prevNext.append(keyValList[i])\n",
    "        firstLevel = False\n",
    "    else:\n",
    "        for prefix, newword, prob in prevNext:\n",
    "            if len(prefix) >= maxLength-1:\n",
    "                terminateMe = True\n",
    "                break\n",
    "            if newword not in transitionMatrix:\n",
    "                # this only happens when we randomly select an end word at the start\n",
    "                continue\n",
    "            keys = transitionMatrix[newword].keys()\n",
    "            keyValList = zip([prefix + [newword] for i in range(len(keys))], keys, constructBayesDist(prefix, keys))\n",
    "            keyValList.sort(key=operator.itemgetter(2), reverse=True)\n",
    "            newBeamWidth = min(len(keyValList), beamWidth)\n",
    "            for i in range(newBeamWidth):\n",
    "                lastchar = keyValList[i][1][len(keyValList[i][1])-1]\n",
    "                if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "                    finishedCandidates.append(keyValList[i])\n",
    "                else:\n",
    "                    currNext.append(keyValList[i])\n",
    "                    \n",
    "        # sort results of the next depth and remove elements outside the beam\n",
    "        currNext.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        if len(currNext) > beamWidth:\n",
    "            del currNext[beamWidth:]\n",
    "\n",
    "        prevNext = deepcopy(currNext)\n",
    "        currNext = []\n",
    "\n",
    "# sort finished candidates by clickbait probability and output the results\n",
    "finishedCandidates = filter(lambda x: len(x[0]) >= minLength, finishedCandidates)\n",
    "finishedCandidates.sort(key=operator.itemgetter(2), reverse=True)\n",
    "newBeamWidth = min(len(finishedCandidates), numTitles)\n",
    "for i in range(newBeamWidth):\n",
    "    print \" \".join(finishedCandidates[i][0] + [finishedCandidates[i][1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
