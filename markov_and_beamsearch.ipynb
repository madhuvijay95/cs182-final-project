{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model and Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4643 clickbait articles.\n",
      "   Unnamed: 0                                      article_title                                        article_url  clickbait    source\n",
      "0           0             23 Life Lessons Cosmo Kramer Taught Us  /javiermoreno/ife-lessons-you-learned-from-cos...          1  Buzzfeed\n",
      "1           1          32 Men On TV Who Made You Thirsty In 2014  /erinlarosa/32-tv-men-who-made-you-thirsty-in-...          1  Buzzfeed\n",
      "2           2          Hilary Duff Was The Walking Queen Of 2014  /lyapalater/hilary-duff-was-the-walking-queen-...          1  Buzzfeed\n",
      "3           3        25 Reasons Wine Is Definitely Your Soulmate  /emleschh/25-reasons-why-wine-is-your-soulmate...          1  Buzzfeed\n",
      "4           4  This Master Carver Making Pliers From One Stic...          /norbertobriceno/ernest-macguyver-warther          1  Buzzfeed\n"
     ]
    }
   ],
   "source": [
    "# import data from augmented.csv\n",
    "filename = 'augmented.csv'\n",
    "augmented_df = pd.read_csv(filename)\n",
    "gh_data = augmented_df[augmented_df['clickbait'] == 1]\n",
    "\n",
    "print 'There are', len(gh_data), 'clickbait articles.'\n",
    "print gh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Sanitize titles for Markov model, output each token on separate line in article_titles.txt ###\n",
    "\n",
    "def removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def removeQuotations(s):\n",
    "    counter = 0\n",
    "    for i in s:\n",
    "        if i == '\\\"':\n",
    "            counter += 1\n",
    "    return not (counter == 2 or counter == 0)\n",
    "\n",
    "def removeParens(s):\n",
    "    counter = 0\n",
    "    for i in s:\n",
    "        if i == '(' or i == ')':\n",
    "            counter += 1\n",
    "    return not (counter == 2 or counter == 0)\n",
    "\n",
    "titles = open('article_titles.txt', 'w')\n",
    "counter = 0\n",
    "for i in gh_data['article_title']:\n",
    "    nonAscii = removeNonAscii(i)\n",
    "    strlist = list(nonAscii)\n",
    "    lastchar = strlist[len(strlist) - 1]\n",
    "    if lastchar != '.' and lastchar != '!' and lastchar != '?':\n",
    "        strlist.append('.')\n",
    "    nonAscii = ''.join(strlist)\n",
    "    tokens = nonAscii.split()\n",
    "    if len(tokens) < 4:\n",
    "        continue\n",
    "    for token in tokens:\n",
    "        if removeQuotations(token):\n",
    "            token = token.replace('\\\"', \"\")\n",
    "        if removeParens(token):\n",
    "            token = token.replace('(', \"\")\n",
    "            token = token.replace(')', \"\")\n",
    "        if token.isupper():\n",
    "            if len(token) != 1 or token[0] == 'A':\n",
    "                token = token.lower()\n",
    "                tokenlist = list(token)\n",
    "                tokenlist[0] = tokenlist[0].upper()\n",
    "                token = \"\".join(tokenlist)\n",
    "        tokenlist = list(token)\n",
    "        tokenlist.append('\\n')\n",
    "        titles.write(''.join(tokenlist))\n",
    "titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53897\n",
      "5705\n"
     ]
    }
   ],
   "source": [
    "### Construct word-to-int mapping, set of existing titles for comparison below.  ###\n",
    "\n",
    "words = open('article_titles.txt', 'r')\n",
    "\n",
    "wordMap = {}\n",
    "reverseWordMap = {}\n",
    "existingTitles = []\n",
    "currentTitle = []\n",
    "totalWords = 0\n",
    "for line in words:\n",
    "    # Construct mapping of words to ints\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line not in wordMap:\n",
    "        wordMap[line] = len(wordMap)\n",
    "        reverseWordMap[wordMap[line]] = line\n",
    "    \n",
    "    # Construct set of article titles to test for similarity\n",
    "    currentTitle.append(wordMap[line])\n",
    "    lastchar = line[len(line)-1]\n",
    "    if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "        existingTitles.append(currentTitle)\n",
    "        totalWords += len(currentTitle)\n",
    "        currentTitle = []\n",
    "        \n",
    "words.close()\n",
    "\n",
    "print totalWords\n",
    "print len(existingTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Construct transition matrix of words, word frequency (mapping from word to count), and starter word frequency ###\n",
    "\n",
    "words = open('article_titles.txt', 'r')\n",
    "transitionMatrix = {}\n",
    "wordFrequency = {}\n",
    "beginnerFrequency = {}\n",
    "\n",
    "prevToken = None\n",
    "nextWordStart = True\n",
    "for line in words:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    \n",
    "    if line not in wordFrequency:\n",
    "        wordFrequency[line] = 0\n",
    "    wordFrequency[line] += 1\n",
    "    \n",
    "    if nextWordStart:\n",
    "        if line not in beginnerFrequency:\n",
    "            beginnerFrequency[line] = 0\n",
    "        beginnerFrequency[line] += 1\n",
    "        nextWordStart = False\n",
    "    \n",
    "    if prevToken == None:\n",
    "        prevToken = line\n",
    "    else:\n",
    "        if prevToken not in transitionMatrix:\n",
    "            transitionMatrix[prevToken] = {}\n",
    "        if line not in transitionMatrix[prevToken]:\n",
    "            transitionMatrix[prevToken][line] = 0\n",
    "        transitionMatrix[prevToken][line] += 1\n",
    "    \n",
    "    lastchar = line[len(line)-1]\n",
    "    if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "        prevToken = None\n",
    "        nextWordStart = True\n",
    "    else:\n",
    "        prevToken = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper functions for Markov model ###\n",
    "\n",
    "# Construct successor probability dist based on global word frequency\n",
    "def constructDist(keys):\n",
    "    frequencies = []\n",
    "    for key in keys:\n",
    "        frequencies.append(wordFrequency[key])\n",
    "    return map(lambda x: x / float(sum(frequencies)), frequencies)\n",
    "\n",
    "# Construct successor probability dist based on local word frequency\n",
    "def constructLocalDist(keys, frequencyTable):\n",
    "    frequencies = []\n",
    "    for key in keys:\n",
    "        frequencies.append(frequencyTable[key])\n",
    "    return map(lambda x: x / float(sum(frequencies)), frequencies)\n",
    "\n",
    "# Find fraction of title2 words appear in title1 (i.e. compare bags of words)\n",
    "def compareWords(title1, title2):\n",
    "    counter = 0\n",
    "    for i in title2:\n",
    "        if i in title1:\n",
    "            counter += 1\n",
    "    return float(counter) / len(title2)\n",
    "\n",
    "# Check for similarity against existing titles, return score of \"best\" (as in, most similar) existing title\n",
    "def checkTable(phrase):\n",
    "    # generate the list\n",
    "    newPhrase = []\n",
    "    tokens = phrase\n",
    "    for token in tokens:\n",
    "        newPhrase.append(wordMap[token])\n",
    "    \n",
    "    # Check against all preexisting titles\n",
    "    bestScore = 0\n",
    "    mostSimilar = None\n",
    "    for title in existingTitles:\n",
    "        score = compareWords(title, newPhrase)\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            mostSimilar = title\n",
    "    \n",
    "    return bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could Doom In Her Show's Twitter Chat.\n",
      "Singer Found Grandpa's Secret Island Was ...\n",
      "'X-Men' Knocks Down by Andrew Rannells on Sherri Shepherd's Reaction GIFs From Cuba?\n",
      "Famous Last Steal Your New Season 5 Common Phrases That Hurt To Leave.\n",
      "How Bad Valentine's Days, But It's Funnier Than Men?\n",
      "26 Most Bitchin' Way Your Facebook Takes Selfies That Theyre Sure #5 .\n",
      "Just Really Think This Pop Song 'for the Knot in Nyc.\n",
      "Burger King Bathroom as Human Being...And This Day.\n",
      "#12 May Not Going Wild Side.\n",
      "Sunburns That Theyre Planning On Comedy And Look Like.\n",
      "Stories of Pit Bull Has Died.\n",
      "So Clear I'm That There Should Do This.\n",
      "Prepare You Hours After Brief Illness.\n",
      "Peter, And Just Wait Until August, Adds Flavor Too!\n",
      "Alert: Has Special Premiered to Heaven'.\n",
      "Bruno Mars Would Skyrocket If These Pics.\n",
      "Uplifting: This Green Screens Works...And Now That Bad-Ass Kill Them.\n",
      "Prince George and Jimmy Fallon A Sample Size Means.\n",
      "Survivor Contestant Caleb Johnson Severs Penis To Sleep.\n",
      "Jane Fonda in Spoof Honeymoon in Pittsburgh Hotel.\n",
      "Fighting Chance Against Gawker Dismissed From Police for 'Ant-Man'.\n",
      "Coldplay Releases First They Looked When She Let This Normal-Looking Bike?\n",
      "What's Growing Up To Her Givenchy Wedding Ring From Gone Terribly Wrong...OMG Run!\n",
      "DWTS's Partner Brittany Kerr After Filming.\n",
      "Kfc Just Ruined By Putting Flowers In NYC...This Is Freaking Day.\n",
      "Sex Seem Great About Princess Diana After Five Golden Globes.\n",
      "Problem With A Love Their Babies For Missing Malaysian Airlines Flight and Loved.\n",
      "Between Pilots and 'The Bfg' Get Divorced On Deceased Teen.\n",
      "You've Definitely Got Trolled Pharrell's \"Happy\".\n",
      "Rep: Shia LaBeouf Receiving Treatment During Massive Storms .\n",
      "Meg Ryan Lewis: My Grandma ...\n",
      "Singer Dropping The Cutest Animal Drawings From Cuba?\n",
      "Did 50 Years Old And Of Quokka?\n",
      "Morrissey Cancels Another 18 Dog in May.\n",
      "Apple Pulls The Wild For Everyday Lives.\n",
      "Life Change...But His Pocket Every Bizarre Jobs Will Set Of #9.\n",
      "Taking Photos Like Disney Channel Cancels All Time...#1 Is Awesome.\n",
      "Fifa Agrees To Survive After Omitting Jay-Z's Master Carver Making Racist Joke.\n",
      "Zach Braff's Response Is Able To Suspect Mrs.\n",
      "Khloe Kardashian Skipped Kim & Drew Self Portraits Too Hipster.\n",
      "Alexa Ray Joel and Civil War In Season 4 Brain-Busting Riddles?\n",
      "Wtf Were Sick Of Places That Still Chooses To Build A Blender Purchase Today.\n",
      "Ford Rushed to Michael Jace's Wife.\n",
      "Census: The Finished His Dog Might Just Tried Wearing Camoflauge.\n",
      "Facebook Giraffe Kissing Kanye West.\n",
      "Love Of Mono Twins Hold Amid His Birthday.\n",
      "#RainbowPorn Is Weird Things Youd Never Get So Romantic Your Loudly Growling Stomach.\n",
      "Orange Is Switching States Is Getting Their Food.\n",
      "Toby Kebbell Cast Celebrates 25th Anniversary Of Little Over Peaches Geldof Mourned by the Premiere.\n",
      "Pandas, Emus, and Diamond Ring at Museum.\n"
     ]
    }
   ],
   "source": [
    "### Random walk through transition matrix, start from a starter word and end at punctuation (., !, ?)\n",
    "\n",
    "# tweakable parameters\n",
    "\n",
    "# min/max numbers of tokens per title\n",
    "minLength = 5\n",
    "maxLength = 15\n",
    "\n",
    "# number of titles to generate\n",
    "numTitles = 50\n",
    "\n",
    "# maximum allowed proportion of shared words b/t generated and preexisting title\n",
    "similarityThreshold = 0.5\n",
    "\n",
    "# don't touch these\n",
    "phrase = []\n",
    "prevWord = None\n",
    "counter = 0\n",
    "while True:\n",
    "    while True:\n",
    "        if prevWord == None:\n",
    "                keys = beginnerFrequency.keys()\n",
    "                word = np.random.choice(keys, 1, constructLocalDist(keys, beginnerFrequency))\n",
    "                phrase.append(word[0])\n",
    "                prevWord = word[0]\n",
    "        else:\n",
    "            if prevWord not in transitionMatrix:\n",
    "                phrase = []\n",
    "                prevWord = None\n",
    "            else:\n",
    "                keys = transitionMatrix[prevWord].keys()\n",
    "                word = np.random.choice(keys, 1, constructLocalDist(keys, transitionMatrix[prevWord]))\n",
    "                prevWord = word[0]\n",
    "                phrase.append(word[0])\n",
    "                lastchar = word[0][len(word[0])-1]\n",
    "                if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "                    break\n",
    "    if len(phrase) < minLength or len(phrase) > maxLength:\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    elif checkTable(phrase) > similarityThreshold:\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    else:\n",
    "        counter += 1\n",
    "        print \" \".join(phrase)\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    if counter > numTitles - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Initialize Naive Bayes model ###\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from naive_bayes import NaiveBayes\n",
    "\n",
    "filename = 'augmented.csv'\n",
    "augmented_df = pd.read_csv(filename)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(augmented_df['article_title'])\n",
    "y = np.array(augmented_df['clickbait'])\n",
    "\n",
    "nb_madhu = NaiveBayes()\n",
    "nb_madhu.fit(X, y, vectorizer.vocabulary_, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mind-Blowing Photos We Make Us Why These Parents Actually The World.\n",
      "Everyday People Absolutely Adorable Moments To It Your Old Photos I Can Make More Joyless.\n",
      "Mind-Blowing Photos We Make Us Why These Animals Were Your Dog Moments.\n",
      "Mind-Blowing Photos We Make Us Why These Animals Were Your Personality?\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Holidays.\n",
      "Mind-Blowing Photos We Make Us Why These Hats.\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Nba In Quicksand.\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Nba In Gaza.\n",
      "Everyday People Absolutely Adorable Moments To These 11 Ways Your Bff?\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Second Most Incredible Weightloss Feats.\n",
      "Mind-Blowing Photos We Make Us Why These Dogs In Jail After One This Happened...\n",
      "Everyday People Absolutely Adorable Moments To Worst.\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Woods To Be ...\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Woods To Hospital.\n",
      "Everyday People Absolutely Adorable Moments To These Exist.\n",
      "Everyday People Absolutely Adorable Moments To It Your Old Photos I Turned Into a Man.\n",
      "Everyday People Absolutely Adorable Moments To These 11 Ways Your Sh*t.\n",
      "Mind-Blowing Photos Really Looks Are People You'd Get You Wish My Girls Is One Season.\n",
      "Everyday People Absolutely Adorable Moments To These 11 Ways Your Childhood.\n",
      "Mind-Blowing Photos We Make Us Why These Parents Actually The Woods To Be Using.\n",
      "CPU times: user 2min 37s, sys: 11.7 s, total: 2min 49s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Beam search w/NB heuristic ###\n",
    "\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from math import exp\n",
    "\n",
    "# tweakable parameters\n",
    "minLength = 5\n",
    "maxLength = 15\n",
    "numTitles = 20\n",
    "beamWidth = 50\n",
    "\n",
    "# Calculate NB probabilities\n",
    "def constructBayesDist(phrase, keys):\n",
    "    probs = []\n",
    "    for key in keys:\n",
    "        newPhrase = phrase + [key]\n",
    "        result = nb_madhu.predict_proba(vectorizer.transform([\" \".join(newPhrase)]))\n",
    "        prob = result[0][1]\n",
    "        probs.append(prob)\n",
    "    # randomly weight each probability\n",
    "    probs = map(lambda x: x * random.uniform(.6, 1.), probs)\n",
    "    return probs\n",
    "\n",
    "prevNext = []\n",
    "currNext = []\n",
    "finishedCandidates = []\n",
    "firstLevel = True\n",
    "terminateMe = False\n",
    "\n",
    "while not terminateMe:\n",
    "    if firstLevel == True:\n",
    "        keys = beginnerFrequency.keys()\n",
    "        keyValList = zip([[] for i in range(len(keys))], keys, constructBayesDist([], keys))\n",
    "        keyValList.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        newBeamWidth = min(len(keyValList), beamWidth)\n",
    "        for i in range(newBeamWidth):\n",
    "            prevNext.append(keyValList[i])\n",
    "        firstLevel = False\n",
    "    else:\n",
    "        for prefix, newword, prob in prevNext:\n",
    "            if len(prefix) >= maxLength-1:\n",
    "                terminateMe = True\n",
    "                break\n",
    "            if newword not in transitionMatrix:\n",
    "                # this only happens when we randomly select an end word at the start\n",
    "                continue\n",
    "            keys = transitionMatrix[newword].keys()\n",
    "            keyValList = zip([prefix + [newword] for i in range(len(keys))], keys, constructBayesDist(prefix, keys))\n",
    "            keyValList.sort(key=operator.itemgetter(2), reverse=True)\n",
    "            newBeamWidth = min(len(keyValList), beamWidth)\n",
    "            for i in range(newBeamWidth):\n",
    "                lastchar = keyValList[i][1][len(keyValList[i][1])-1]\n",
    "                if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "                    finishedCandidates.append(keyValList[i])\n",
    "                else:\n",
    "                    currNext.append(keyValList[i])\n",
    "                    \n",
    "        # sort results of the next depth and remove elements outside the beam\n",
    "        currNext.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        if len(currNext) > beamWidth:\n",
    "            del currNext[beamWidth:]\n",
    "\n",
    "        prevNext = deepcopy(currNext)\n",
    "        currNext = []\n",
    "\n",
    "# sort finished candidates by clickbait probability and output the results\n",
    "finishedCandidates = filter(lambda x: len(x[0]) >= minLength, finishedCandidates)\n",
    "finishedCandidates.sort(key=operator.itemgetter(2), reverse=True)\n",
    "newBeamWidth = min(len(finishedCandidates), numTitles)\n",
    "for i in range(newBeamWidth):\n",
    "    print \" \".join(finishedCandidates[i][0] + [finishedCandidates[i][1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
