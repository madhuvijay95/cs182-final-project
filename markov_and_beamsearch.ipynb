{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 182: Mike\n",
    "\n",
    "Markov model, Beam search w/NB classifier as heuristic, Markovify (https://github.com/jsvine/markovify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4643 clickbait articles.\n",
      "   Unnamed: 0                                      article_title                                        article_url  clickbait    source\n",
      "0           0             23 Life Lessons Cosmo Kramer Taught Us  /javiermoreno/ife-lessons-you-learned-from-cos...          1  Buzzfeed\n",
      "1           1          32 Men On TV Who Made You Thirsty In 2014  /erinlarosa/32-tv-men-who-made-you-thirsty-in-...          1  Buzzfeed\n",
      "2           2          Hilary Duff Was The Walking Queen Of 2014  /lyapalater/hilary-duff-was-the-walking-queen-...          1  Buzzfeed\n",
      "3           3        25 Reasons Wine Is Definitely Your Soulmate  /emleschh/25-reasons-why-wine-is-your-soulmate...          1  Buzzfeed\n",
      "4           4  This Master Carver Making Pliers From One Stic...          /norbertobriceno/ernest-macguyver-warther          1  Buzzfeed\n"
     ]
    }
   ],
   "source": [
    "# import data from augmented.csv\n",
    "filename = 'augmented.csv'\n",
    "augmented_df = pd.read_csv(filename)\n",
    "gh_data = augmented_df[augmented_df['clickbait'] == 1]\n",
    "\n",
    "print 'There are', len(gh_data), 'clickbait articles.'\n",
    "print gh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Sanitize titles for Markov model, output each token on separate line in article_titles.txt ###\n",
    "\n",
    "def removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def removeQuotations(s):\n",
    "    counter = 0\n",
    "    for i in s:\n",
    "        if i == '\\\"':\n",
    "            counter += 1\n",
    "    return not (counter == 2 or counter == 0)\n",
    "\n",
    "def removeParens(s):\n",
    "    counter = 0\n",
    "    for i in s:\n",
    "        if i == '(' or i == ')':\n",
    "            counter += 1\n",
    "    return not (counter == 2 or counter == 0)\n",
    "\n",
    "titles = open('article_titles.txt', 'w')\n",
    "counter = 0\n",
    "for i in gh_data['article_title']:\n",
    "    nonAscii = removeNonAscii(i)\n",
    "    strlist = list(nonAscii)\n",
    "    lastchar = strlist[len(strlist) - 1]\n",
    "    if lastchar != '.' and lastchar != '!' and lastchar != '?':\n",
    "        strlist.append('.')\n",
    "    nonAscii = ''.join(strlist)\n",
    "    tokens = nonAscii.split()\n",
    "    if len(tokens) < 4:\n",
    "        continue\n",
    "    for token in tokens:\n",
    "        if removeQuotations(token):\n",
    "            token = token.replace('\\\"', \"\")\n",
    "        if removeParens(token):\n",
    "            token = token.replace('(', \"\")\n",
    "            token = token.replace(')', \"\")\n",
    "        if token.isupper():\n",
    "            if len(token) != 1 or token[0] == 'A':\n",
    "                token = token.lower()\n",
    "                tokenlist = list(token)\n",
    "                tokenlist[0] = tokenlist[0].upper()\n",
    "                token = \"\".join(tokenlist)\n",
    "        tokenlist = list(token)\n",
    "        tokenlist.append('\\n')\n",
    "        titles.write(''.join(tokenlist))\n",
    "titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53897\n",
      "5705\n"
     ]
    }
   ],
   "source": [
    "### Construct word-to-int mapping, set of existing titles for comparison below.  ###\n",
    "### Also this is kinda dumb but markovify expects 1 line == 1 sentence so write to markovify_titles.txt ###\n",
    "\n",
    "words = open('article_titles.txt', 'r')\n",
    "fullTitles = open('markovify_titles.txt', 'w')\n",
    "\n",
    "wordMap = {}\n",
    "reverseWordMap = {}\n",
    "existingTitles = []\n",
    "currentTitle = []\n",
    "totalWords = 0\n",
    "for line in words:\n",
    "    # Construct mapping of words to ints\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line not in wordMap:\n",
    "        wordMap[line] = len(wordMap)\n",
    "        reverseWordMap[wordMap[line]] = line\n",
    "    \n",
    "    # Construct set of article titles to test for similarity\n",
    "    currentTitle.append(wordMap[line])\n",
    "    lastchar = line[len(line)-1]\n",
    "    if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "        existingTitles.append(currentTitle)\n",
    "        totalWords += len(currentTitle)\n",
    "        fullTitles.write(\" \".join(map(lambda x: reverseWordMap[x], currentTitle) + ['\\n']))\n",
    "        currentTitle = []\n",
    "        \n",
    "words.close()\n",
    "fullTitles.close()\n",
    "\n",
    "print totalWords\n",
    "print len(existingTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Construct transition matrix of words, word frequency (mapping from word to count), and starter word frequency ###\n",
    "\n",
    "words = open('article_titles.txt', 'r')\n",
    "transitionMatrix = {}\n",
    "wordFrequency = {}\n",
    "beginnerFrequency = {}\n",
    "\n",
    "prevToken = None\n",
    "nextWordStart = True\n",
    "for line in words:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    \n",
    "    if line not in wordFrequency:\n",
    "        wordFrequency[line] = 0\n",
    "    wordFrequency[line] += 1\n",
    "    \n",
    "    if nextWordStart:\n",
    "        if line not in beginnerFrequency:\n",
    "            beginnerFrequency[line] = 0\n",
    "        beginnerFrequency[line] += 1\n",
    "        nextWordStart = False\n",
    "    \n",
    "    if prevToken == None:\n",
    "        prevToken = line\n",
    "    else:\n",
    "        if prevToken not in transitionMatrix:\n",
    "            transitionMatrix[prevToken] = {}\n",
    "        if line not in transitionMatrix[prevToken]:\n",
    "            transitionMatrix[prevToken][line] = 0\n",
    "        transitionMatrix[prevToken][line] += 1\n",
    "    \n",
    "    lastchar = line[len(line)-1]\n",
    "    if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "        prevToken = None\n",
    "        nextWordStart = True\n",
    "    else:\n",
    "        prevToken = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper functions for Markov model ###\n",
    "\n",
    "# Construct successor probability dist based on global word frequency\n",
    "def constructDist(keys):\n",
    "    frequencies = []\n",
    "    for key in keys:\n",
    "        frequencies.append(wordFrequency[key])\n",
    "    return map(lambda x: x / float(sum(frequencies)), frequencies)\n",
    "\n",
    "# Construct successor probability dist based on local word frequency\n",
    "def constructLocalDist(keys, frequencyTable):\n",
    "    frequencies = []\n",
    "    for key in keys:\n",
    "        frequencies.append(frequencyTable[key])\n",
    "    return map(lambda x: x / float(sum(frequencies)), frequencies)\n",
    "\n",
    "# Find fraction of title2 words appear in title1 (i.e. compare bags of words)\n",
    "def compareWords(title1, title2):\n",
    "    counter = 0\n",
    "    for i in title2:\n",
    "        if i in title1:\n",
    "            counter += 1\n",
    "    return float(counter) / len(title2)\n",
    "\n",
    "# Check for similarity against existing titles, return score of \"best\" (as in, most similar) existing title\n",
    "def checkTable(phrase):\n",
    "    # generate the list\n",
    "    newPhrase = []\n",
    "    tokens = phrase\n",
    "    for token in tokens:\n",
    "        newPhrase.append(wordMap[token])\n",
    "    \n",
    "    # Check against all preexisting titles\n",
    "    bestScore = 0\n",
    "    mostSimilar = None\n",
    "    for title in existingTitles:\n",
    "        score = compareWords(title, newPhrase)\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            mostSimilar = title\n",
    "    \n",
    "    return bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing But That Even Think They're Still Adorable.\n",
      "'Dynasty' Star Allison Williams Angers Fans Reportedly Was Rear-Ended Him...Then They Won.\n",
      "Katherine Webb Is Breaking a Salty Snack.\n",
      "Katy Perry Launches Own Existence...I Am Alive .\n",
      "Determined To Realize I've Ever To Hack: 11 Times Starbucks Failed To Them.\n",
      "'Glee' Departure, Says They Wrote Notes That Scout Balances On Bid Day.\n",
      "Partner Brittany Murphy's Final Rose': Nick Carter Marries Lauren Kitt.\n",
      "Leonardo DiCaprio Felt like Nick Claims Yale Threatened Forced To Acting...And These Trains.\n",
      "Arnold Schwarzenegger How Bad For Men Alive Around Us .\n",
      "Didn't Even Seem Boring....But When I Would Love It.\n",
      "People Need No Longer Attending Her Husband.\n",
      "Oscar-Nominated Filmmaker Making Pliers From Complications With Knowledge.\n",
      "Rowling's 'The Bachelorette', Calls Andi Dorfman & Its Alibaba Winnings To Mars.\n",
      "Met Until This Painter's Breathtaking Photographs That Came To Angsty Teenagers.\n",
      "X-Men: Days Are Now is Hotttt!\n",
      "'Pretty Little Kittens Who Just Crayons Will Bring Their Lives.\n",
      "Reportedly Cheated After A Nirvana Joined by Cardiovascular Disease.\n",
      "Five-Minute Video Removed Amidst Racism in Different Ethnicities.\n",
      "Kit Harington Thinks Its Official: Anthony Hopkins Was Naughty Dogs Being Different.\n",
      "Seinfeld Reunion Photo of Justice'.\n",
      "Rowlings New Black' Gets Asked 12 Joan Rivers To Expect.\n",
      "Gisele Bundchen and Ashton Kutcher, Matthew Paetz.\n",
      "Anita Baker Wanted to Show Looks Good'.\n",
      "Beyonc Lyrics of Water So Many People Do #6?\n",
      "Muscles So Much More Socially Awkward Beings..I Can't Look What Looked Like?\n",
      "Davis Dies at 'The Sinister Six' Line-Up of Bathrooms, But We Hope That One.\n",
      "Steve Perry Teases Marvel's Thor Reintroduced as Coach Just Completed...See Why We'll Miss Paul Walker.\n",
      "#RainbowPorn Is Perfectly Sassy Animals A Full-Time Host?\n",
      "Kate Winslet Gets Third Season 7 Before-and-After Makeup Artist Gave Zero Chemistry.\n",
      "Guaranteed To Pet Animals, But Wait Till You Missed By Not Believe The Spirit.\n",
      "Florida State Back Is Starting Her Fingers To Procreate.\n",
      "Kate Winslet, and Mustaches: You Realize.\n",
      "Insane Balloon Creations Are Way Of All.\n",
      "Dre and Songs Ranked From Cardiovascular Disease.\n",
      "29 Times Our Countrys Lack Of Dial-Up Speeds.\n",
      "In All Time...#9 Is in Sheer Outfit in Jail.\n",
      "Chelsea Handler Bids Farewell to Know If Real Reason.\n",
      "Brandy and Pip Andersen Join Judd Apatow's Comedy 'Baskets' Ordered to 3 Days.\n",
      "3 Pugs That Looked Before L'Wren Scott Asheton Dies After 2000 For Failing Hilariously.\n",
      "Ukraine's Human On This 6-Year-Old The Chair Is Melting My Hero.\n",
      "Miss Hannigan From Love Actually Found Grandpa's Secret Weapon.\n",
      "Britney Spears Singing Without Wedding Ring from George Clooney Slams Rihanna for Kim Kardashian.\n",
      "Fleetwood Mac Wiseman, Ronnie Milsap to Reattach Andre Johnson Reportedly Working On.\n",
      "Shameful: The Awful Things All Game In 3 Teaser Unleashed.\n",
      "27 Parents Being Strangely Specific Museums You Eat Certain Foods...#16 Is Unlike Anything.\n",
      "Strap A Farm With One Stick Of Accurate.\n",
      "#24 Is Experiencing Right Now Incredible.\n",
      "Remember Reading These Be Unseen.\n",
      "Nothing on Real Story Behind The Spookiest Of Puppies Your Problems.\n",
      "Fifty Shades Preview Clip Reveals Mother's Day.\n"
     ]
    }
   ],
   "source": [
    "### Random walk through transition matrix, start from a starter word and end at punctuation (., !, ?)\n",
    "\n",
    "# tweakable parameters\n",
    "\n",
    "# min/max numbers of tokens per title\n",
    "minLength = 5\n",
    "maxLength = 15\n",
    "\n",
    "# number of titles to generate\n",
    "numTitles = 50\n",
    "\n",
    "# maximum allowed proportion of shared words b/t generated and preexisting title\n",
    "similarityThreshold = 0.5\n",
    "\n",
    "# don't touch these\n",
    "phrase = []\n",
    "prevWord = None\n",
    "counter = 0\n",
    "while True:\n",
    "    while True:\n",
    "        if prevWord == None:\n",
    "                keys = beginnerFrequency.keys()\n",
    "                word = np.random.choice(keys, 1, constructLocalDist(keys, beginnerFrequency))\n",
    "                phrase.append(word[0])\n",
    "                prevWord = word[0]\n",
    "        else:\n",
    "            if prevWord not in transitionMatrix:\n",
    "                phrase = []\n",
    "                prevWord = None\n",
    "            else:\n",
    "                keys = transitionMatrix[prevWord].keys()\n",
    "                word = np.random.choice(keys, 1, constructLocalDist(keys, transitionMatrix[prevWord]))\n",
    "                prevWord = word[0]\n",
    "                phrase.append(word[0])\n",
    "                lastchar = word[0][len(word[0])-1]\n",
    "                if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "                    break\n",
    "    if len(phrase) < minLength or len(phrase) > maxLength:\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    elif checkTable(phrase) > similarityThreshold:\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    else:\n",
    "        counter += 1\n",
    "        print \" \".join(phrase)\n",
    "        phrase = []\n",
    "        prevWord = None\n",
    "    if counter > numTitles - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initialize Naive Bayes model ###\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from naive_bayes import NaiveBayes\n",
    "\n",
    "filename = 'augmented.csv'\n",
    "augmented_df = pd.read_csv(filename)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(augmented_df['article_title'])\n",
    "y = np.array(augmented_df['clickbait'])\n",
    "\n",
    "nb_madhu = NaiveBayes()\n",
    "nb_madhu.fit(X, y, vectorizer.vocabulary_, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### Beam search w/NB heuristic ###\n",
    "\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from math import exp\n",
    "\n",
    "# tweakable parameters\n",
    "minLength = 5\n",
    "maxLength = 15\n",
    "numTitles = 20\n",
    "beamWidth = 50\n",
    "\n",
    "# Construct successor probability distribution based on NB probability \n",
    "def constructBayesDist(phrase, keys):\n",
    "    probs = []\n",
    "    for key in keys:\n",
    "        newPhrase = phrase + [key]\n",
    "        result = nb_madhu.predict_proba(vectorizer.transform([\" \".join(newPhrase)]))\n",
    "        prob = result[0][1]\n",
    "        probs.append(prob)\n",
    "    # randomly weight each probability by e^(-.5x), where 0 <= x <= 1\n",
    "    probs = map(lambda x: x * exp(-.5 * random.random()), probs)\n",
    "    return probs\n",
    "\n",
    "prevNext = []\n",
    "currNext = []\n",
    "finishedCandidates = []\n",
    "firstLevel = True\n",
    "terminateMe = False\n",
    "\n",
    "while not terminateMe:\n",
    "    if firstLevel == True:\n",
    "        keys = beginnerFrequency.keys()\n",
    "        keyValList = zip([[] for i in range(len(keys))], keys, constructBayesDist([], keys))\n",
    "        keyValList.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        newBeamWidth = min(len(keyValList), beamWidth)\n",
    "        for i in range(newBeamWidth):\n",
    "            prevNext.append(keyValList[i])\n",
    "        firstLevel = False\n",
    "    else:\n",
    "        for prefix, newword, prob in prevNext:\n",
    "            if len(prefix) >= maxLength-1:\n",
    "                terminateMe = True\n",
    "                break\n",
    "            if newword not in transitionMatrix:\n",
    "                # this only happens when we randomly select an end word at the start\n",
    "                continue\n",
    "            keys = transitionMatrix[newword].keys()\n",
    "            keyValList = zip([prefix + [newword] for i in range(len(keys))], keys, constructBayesDist(prefix, keys))\n",
    "            keyValList.sort(key=operator.itemgetter(2), reverse=True)\n",
    "            newBeamWidth = min(len(keyValList), beamWidth)\n",
    "            for i in range(newBeamWidth):\n",
    "                lastchar = keyValList[i][1][len(keyValList[i][1])-1]\n",
    "                if (lastchar == '.' or lastchar == '?' or lastchar == '!'):\n",
    "                    finishedCandidates.append(keyValList[i])\n",
    "                else:\n",
    "                    currNext.append(keyValList[i])\n",
    "                    \n",
    "        # sort results of the next depth and remove elements outside the beam\n",
    "        currNext.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        if len(currNext) > beamWidth:\n",
    "            del currNext[beamWidth:]\n",
    "\n",
    "        prevNext = deepcopy(currNext)\n",
    "        currNext = []\n",
    "\n",
    "# sort finished candidates by clickbait probability and output the results\n",
    "finishedCandidates = filter(lambda x: len(x[0]) >= minLength, finishedCandidates)\n",
    "finishedCandidates.sort(key=operator.itemgetter(2), reverse=True)\n",
    "newBeamWidth = min(len(finishedCandidates), numTitles)\n",
    "for i in range(newBeamWidth):\n",
    "    print \" \".join(finishedCandidates[i][0] + [finishedCandidates[i][1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Cakes That Look . The Kitten Soldiers Of World War Ii Ghost Photographs.\n",
      "This Is What It's Really Shocking.\n",
      "23 Times Louis Tomlinson Smoking a Joint.\n",
      "Chris Hemsworth and Elsa Pataky Giving Birth Over And Over Without Stopping Since Last Month!\n",
      "13 Pics of Little Kids Who Dress Better Than The Average Biopic.\n"
     ]
    }
   ],
   "source": [
    "### Use markovify library https://github.com/jsvine/markovify ###\n",
    "\n",
    "import markovify\n",
    "\n",
    "with open(\"markovify_titles.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# state_size=3 results look cleaner, but are less novel and more likely to closely resemble a headline in the corpus.\n",
    "text_model = markovify.Text(text, state_size=2)\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
